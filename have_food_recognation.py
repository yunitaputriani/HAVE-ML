# -*- coding: utf-8 -*-
"""HAVE food recognation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DI4PhGcoR4G8qGwwlAmW4UB4LGLfnOkf

**Import Libraries**
"""

pip install tensorflow

import os
import zipfile
import random
import shutil
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from shutil import copyfile
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from ipywidgets import FileUpload
from PIL import Image
import io

"""**Upload and Extract Dataset**"""

# Unggah file ZIP dataset
uploadfile = '/content/foods_data.zip'

# Ekstrak file ZIP
with zipfile.ZipFile(uploadfile, 'r') as zip_ref:
    zip_ref.extractall('/content/')

# Menetapkan jalur sumber data
source_path = '/content/LIST MAKANAN'

root_dir = os.path.expanduser('~/food')

"""**Creating Directories for Train, Validation, and Test Sets**"""

# Menghapus direktori yang kosong untuk mencegah FileExistsError jika fungsi dijalankan beberapa kali
if os.path.exists(root_dir):
  shutil.rmtree(root_dir)

def create_train_val_dirs(root_path):

  # Membuat direktori untuk the train and test sets
  # Membuat path untuk direktori
  os.makedirs(root_path, exist_ok=True)
  train_dir = os.path.join(root_path, 'training')
  val_dir = os.path.join(root_path, 'validation')
  test_dir = os.path.join(root_path, 'testing')

  # Membuat direktori untuk setiap kelas di dalam direktori
  foods =  ['egg','rice','sweet tea','waffle','orange juice','pancake']
  for x in foods:
    os.makedirs(os.path.join(train_dir, x), exist_ok=True)
    os.makedirs(os.path.join(val_dir, x), exist_ok=True)
    os.makedirs(os.path.join(test_dir, x), exist_ok=True)

try:
  create_train_val_dirs(root_path=root_dir)
except FileExistsError:
  print("You should not be seeing this since the upper directory is removed beforehand")

"""**Splitting Data into Train, Validation, and Test Sets**"""

def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, TESTING_DIR, SPLIT_SIZE, SPLIT_SIZE_TEST):
  file_list = [os.path.join(SOURCE_DIR, file) for file in os.listdir(SOURCE_DIR) if os.path.getsize(os.path.join(SOURCE_DIR, file)) > 0]

  # Menentukan jumlah makanan yang akan digunakan untuk data latih
  train_num_foods = int(round(len(file_list) * SPLIT_SIZE))
  # Menentukan jumlah makanan yang akan digunakan untuk data latih secara acak
  train_list = random.sample(file_list, train_num_foods)
  # Menyisakan jumlah makanan yang belum digunakan untuk data latih
  remain_list = list(set(file_list) - set(train_list))
  # Menentukan jumlah makanan yang akan digunakan untuk data pengujian
  test_num_foods = int(round(len(remain_list) * SPLIT_SIZE_TEST/(1-SPLIT_SIZE)))
  # Menentukan jumlah makanan yang akan digunakan untuk data pengujian secara acak
  test_list = random.sample(remain_list, test_num_foods)
  # Menyisakan jumlah makanan yang belum digunakan untuk data pengujian sebagai data validasi
  val_list = list(set(remain_list) - set(test_list))

  # Menyalin file-file data train, validation, test ke dalam direktori
  for f in train_list:
    copyfile(f, os.path.join(TRAINING_DIR, os.path.basename(f)))
  for f in test_list:
    copyfile(f, os.path.join(TESTING_DIR, os.path.basename(f)))
  for f in val_list:
    copyfile(f, os.path.join(VALIDATION_DIR, os.path.basename(f)))

TRAINING_DIR = os.path.join(root_dir, "training")
VALIDATION_DIR = os.path.join(root_dir, "validation")
TESTING_DIR = os.path.join(root_dir, "testing")
foods =  ['egg','rice','sweet tea','waffle','orange juice','pancake']

for food in foods:
    SOURCE_DIR_FOOD = os.path.join(source_path, food)
    TRAINING_FOOD_DIR = os.path.join(TRAINING_DIR, food)
    VALIDATION_FOOD_DIR = os.path.join(VALIDATION_DIR, food)
    TESTING_FOOD_DIR = os.path.join(TESTING_DIR, food)

    if not os.path.exists(SOURCE_DIR_FOOD):  # <-- TAMBAHKAN PENGECEKAN INI
        print(f"Folder {SOURCE_DIR_FOOD} tidak ditemukan, pastikan nama folder sesuai dengan yang ada di direktori.")
        continue

    for dir_path in [TRAINING_FOOD_DIR, VALIDATION_FOOD_DIR, TESTING_FOOD_DIR]:
        for file in os.scandir(dir_path):
            os.remove(file.path)

    # Memisahkan data untuk kelas makanan tertentu menjadi data latih, validasi, dan pengujian
    split_data(SOURCE_DIR_FOOD, TRAINING_FOOD_DIR, VALIDATION_FOOD_DIR, TESTING_FOOD_DIR, 0.8, 0.1)

    print(f"\n\nOriginal {food}'s directory has {len(os.listdir(SOURCE_DIR_FOOD))} images")

    # Training and validation splits
    print(f"There are {len(os.listdir(TRAINING_FOOD_DIR))} images of {food} for training")
    print(f"There are {len(os.listdir(VALIDATION_FOOD_DIR))} images of {food} for validation")
    print(f"There are {len(os.listdir(TESTING_FOOD_DIR))} images of {food} for testing")

"""**Data Augmentation and Loading**"""

# Mendefinisikan ukuran gambar yang diinginkan untuk proses pelatihan model
target_size = (128, 128)  # Ukuran gambar yang diinginkan untuk dilatih dari dataset menjadi model.

# Proses Augmentasi Gambar dan Pengelompokan Data
# Data pelatihan akan ditingkatkan dengan variasi seperti rotasi, pergeseran, pemotongan, dan lainnya.
# Gambar-gambar ini akan digunakan untuk melatih model agar lebih adaptif terhadap variasi dalam data.
# Gambar-gambar yang ada juga akan dikelompokkan sesuai dengan kelasnya.
# Memuat gambar-gambar dari direktori data latih dan data validasi.
train_datagen = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,
    rotation_range=10, width_shift_range=0.2, height_shift_range=0.2,
    shear_range=0.1, zoom_range=0.3, horizontal_flip=True, fill_mode='nearest'
)
train_generator = train_datagen.flow_from_directory(
    TRAINING_DIR,
    target_size=target_size,
    batch_size=32,
    class_mode='categorical'
)

validation_datagen = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input
)
validation_generator = validation_datagen.flow_from_directory(
    VALIDATION_DIR,
    target_size=target_size,
    batch_size=32,
    class_mode='categorical'
)

testing_datagen = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input
)
testing_generator = testing_datagen.flow_from_directory(
    TESTING_DIR,
    target_size=target_size,
    batch_size=32,
    class_mode='categorical'
)

"""**Building and Training the Model**"""

basemodel = tf.keras.applications.MobileNetV2(input_shape=target_size + (3,), include_top=False, weights='imagenet')
basemodel.trainable = False

inputs = tf.keras.Input(shape=target_size + (3,))
x = basemodel(inputs, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.Dense(6, activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy'])

history = model.fit(train_generator,
                    epochs=100,
                    validation_data=validation_generator)

"""**Plotting Training and Validation Metrics**"""

# Plot the chart for accuracy and loss on both training and validation
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from sklearn.metrics import accuracy_score, log_loss

# Prediksi label dari data testing
predictions = model.predict(testing_generator)

# Mengambil label sebenarnya dari data testing
true_labels = testing_generator.classes

# Menghitung akurasi dan loss dari data testing
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))
loss = log_loss(true_labels, predictions)

# Plot akurasi
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.show()

# Evaluasi model pada data testing
loss, accuracy = model.evaluate(testing_generator)
print("Testing Accuracy:", accuracy)
print("Testing Loss:", loss)

# Plot akurasi
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.show()

from sklearn.metrics import classification_report, confusion_matrix

# Prediksi label dari data testing
predictions = model.predict(testing_generator)

# Mengambil label sebenarnya dari data testing
true_labels = testing_generator.classes

# Konversi prediksi menjadi label kelas
predicted_labels = np.argmax(predictions, axis=1)

# Mencetak classification report
print("Classification Report:")
print(classification_report(true_labels, predicted_labels))

# Mencetak confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(true_labels, predicted_labels))

"""**Food Calories**"""

# Fungsi untuk memprediksi makanan
def predict_food(image_path, model, class_indices, calories_dict):
    img = load_img(image_path, target_size=(128, 128))
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0) / 255.0

    prediction = model.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)
    predicted_food = list(class_indices.keys())[list(class_indices.values()).index(predicted_class[0])]
    return predicted_food, calories_dict.get(predicted_food, "Informasi kalori tidak tersedia")

# Menyediakan dictionary untuk kalori
calories_dict = {
    'egg': '92 kalori',
    'sweet tea': '260 kalori',
    'rice':'130 kalori',
    'pancake': '277 kalori',
    'waffle':'291 kalori',
    'orange juice':'45 kalori',
}

"""**Saving and Evaluating the Model**"""

!pip install h5py
import h5py

# Saving the trained model as a Keras H5 file.
saved_model_path = "./foods_model.h5"
model.save(saved_model_path)

# Loading the saved model
loaded_model = tf.keras.models.load_model("./foods_model.h5")

# Evaluation
results = loaded_model.evaluate(testing_generator, batch_size=32)
print("test loss, test acc:", results)

"""**Converting the Model to TensorFlow Lite**"""

# Memuat model
loaded_model = tf.keras.models.load_model('foods_model.h5')

# Memuat kelas-kelas yang diperlukan dari generator
class_indices = train_generator.class_indices

# Konversi model ke format TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Simpan model TensorFlow Lite
with open('foods_model.tflite', 'wb') as f:
    f.write(tflite_model)

pip install tflite-support

from tflite_support.metadata_writers import image_classifier
from tflite_support.metadata_writers import writer_utils

# Isi daftar label kelas sesuai dengan model
labels = ['egg', 'rice', 'sweet tea', 'waffle', 'orange juice', 'pancake']

# Tulis daftar label ke dalam file labels.txt
with open('labels.txt', 'w') as f:
    for label in labels:
        f.write(label + '\n')

# Path ke model TFLite
model_path = 'foods_model.tflite'

# Path untuk menyimpan model yang sudah ditambahkan metadata
export_path = 'foods_model_with_metadata.tflite'

# Path ke file label
label_file_path = 'labels.txt'

# Buat object ImageClassifier dengan mean dan std normalisasi
image_classifier_writer = image_classifier.MetadataWriter.create_for_inference(
    writer_utils.load_file(model_path),
    input_norm_mean=[127.5],  # Mean untuk normalisasi
    input_norm_std=[127.5],   # Std untuk normalisasi
    label_file_paths=[label_file_path]  # Path ke file label
)

# Tambahkan metadata ke model
export_model_path = image_classifier_writer.populate()
writer_utils.save_file(export_model_path, export_path)

print("Metadata telah ditambahkan ke model dan disimpan di:", export_path)

"""**Uji coba**"""

# Fungsi untuk menangani pengunggahan file
def on_upload_change(change):
    uploaded_file = change['new']
    if uploaded_file:
        file_info = uploaded_file[list(uploaded_file.keys())[0]]
        content = file_info['content']
        image = Image.open(io.BytesIO(content))
        image_path = "/tmp/uploaded_image.jpg"
        image.save(image_path)

        predicted_food, calories = predict_food(image_path, model, class_indices, calories_dict)
        print("Prediksi Nama Makanan:", predicted_food)
        print("Jumlah Kalori:", calories)

# Widget untuk mengunggah file
upload_widget = FileUpload()
upload_widget.observe(on_upload_change, names='value')
display(upload_widget)

